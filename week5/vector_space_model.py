import re
import os
import unicodedata
import logging
from tqdm import tqdm
from datasets import load_dataset
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec
import pandas as pd
from pyvi import ViTokenizer

# ==========================
# ‚öôÔ∏è C·∫§U H√åNH CHUNG
# ==========================
OUTPUT_DIR = r"D:\Vietnamese_Word2Vec"
os.makedirs(OUTPUT_DIR, exist_ok=True)

FINAL_MODEL_PATH = os.path.join(OUTPUT_DIR, "w2v_vietnamese.model")
CHECKPOINT_PREFIX = os.path.join(OUTPUT_DIR, "w2v_vietnamese_checkpoint")
WORKERS = 15 # S·ªë lu·ªìng CPU ƒë·ªÉ hu·∫•n luy·ªán
# ==========================
# üß© C·∫§U H√åNH LOGGING
# ==========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s : %(levelname)s : %(message)s"
)
logger = logging.getLogger(__name__)

# ==========================
# üîÅ CALLBACK L∆ØU CHECKPOINT
# ==========================
class EpochSaver(CallbackAny2Vec):
    """Callback l∆∞u m√¥ h√¨nh sau m·ªói epoch."""
    def __init__(self, prefix):
        self.prefix = prefix
        self.epoch = 0

    def on_epoch_end(self, model):
        self.epoch += 1
        path = f"{self.prefix}_epoch_{self.epoch}.model"
        model.save(path)
        logger.info(f"[Checkpoint] ‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh sau epoch {self.epoch}: {path}")

# ==========================
# üßπ H√ÄM TI·ªÄN X·ª¨ L√ù CHU·ªñI
# ==========================
_re_non_vn = re.compile(r"[^0-9a-zA-Z√Ä-·ª¥√†-·ªµƒëƒê\s]", re.UNICODE)
_re_multi_space = re.compile(r"\s+")

def normalize_text(text):
    """Chu·∫©n ho√° Unicode v√† lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá."""
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize("NFC", text)
    text = _re_non_vn.sub(" ", text)
    text = _re_multi_space.sub(" ", text)
    return text.strip().lower()

_re_valid_vn = re.compile(
    r"^[a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ]+$",
    re.UNICODE
)

def is_valid_vietnamese_word(word):
    """Ki·ªÉm tra token c√≥ h·ª£p l·ªá ti·∫øng Vi·ªát hay kh√¥ng."""
    return bool(_re_valid_vn.match(word))
# ==========================
# üß± X√ÇY TRIE CHO STOPWORD
# ==========================
def build_trie(phrases):
    """T·∫°o Trie cho stopwords."""
    trie = {}
    max_len = 0
    single_word_set = set()

    for phr in phrases:
        toks = phr.split()
        if not toks:
            continue
        if len(toks) == 1:
            single_word_set.add(toks[0])
            max_len = max(max_len, 1)
            continue
        node = trie
        for tok in toks:
            node = node.setdefault(tok, {})
        node["_end"] = True
        max_len = max(max_len, len(toks))
    return trie, single_word_set, max_len

# ==========================
# üß† TOKENIZE + CLEAN
# ==========================
def tokenize_and_clean_vietnamese(text, trie, single_stop, max_phrase_len):
    """T√°ch t·ª´, lo·∫°i stopword & t·ª´ r√°c."""
    text = normalize_text(text)
    # ‚úÖ D√πng PyVi ƒë·ªÉ t√°ch t·ª´ ƒë√∫ng ng·ªØ c·∫£nh ti·∫øng Vi·ªát
    tokenized = ViTokenizer.tokenize(text)
    # PyVi s·∫Ω t√°ch "ƒëi h·ªçc" -> "ƒëi_h·ªçc", "th√†nh ph·ªë" -> "th√†nh_ph·ªë"
    tokens = [t.replace("_", " ") for t in tokenized.split() if is_valid_vietnamese_word(t.replace("_", " "))]

    filtered = []
    i, n = 0, len(tokens)

    while i < n:
        node = trie
        j = i
        steps = 0
        match_len = 0

        while j < n and steps < max_phrase_len:
            tok = tokens[j]
            if not isinstance(node, dict) or tok not in node:
                break
            node = node[tok]
            j += 1
            steps += 1
            if node.get("_end"):
                match_len = j - i

        if match_len == 0 and tokens[i] not in single_stop:
            filtered.append(tokens[i])

        i += max(match_len, 1)

    return filtered

# ==========================
# üìö STREAM CORPUS
# ==========================
def tokenize_stream(dataset_iter, trie, single_stop, max_len):
    """Sinh d·ªØ li·ªáu tokenized cho Word2Vec."""
    for sample in dataset_iter:
        text = sample.get("text", "")
        toks = tokenize_and_clean_vietnamese(text, trie, single_stop, max_len)
        if toks:
            yield toks

# ==========================
# üöÄ MAIN
# ==========================
def main():
    # ---- 1Ô∏è‚É£ T·∫¢I STOPWORDS ----
    stopword_file = "stopwords.csv"
    df = pd.read_csv(stopword_file)
    stop_phrases = [str(w).strip().replace("_", " ") for w in df["stopwords"].dropna()]
    trie, single_stop, max_phrase_len = build_trie(stop_phrases)
    logger.info(f"‚úÖ ƒê√£ t·∫£i {len(stop_phrases)} c·ª•m stopword.")

    # ---- 2Ô∏è‚É£ KI·ªÇM TRA CHECKPOINT ----
    latest_ckpt = None
    ckpt_epochs = []
    for f in os.listdir(OUTPUT_DIR):
        if f.startswith("w2v_vietnamese_checkpoint_epoch_") and f.endswith(".model"):
            num = int(re.findall(r"epoch_(\d+)", f)[0])
            ckpt_epochs.append((num, os.path.join(OUTPUT_DIR, f)))
    if ckpt_epochs:
        latest_ckpt = sorted(ckpt_epochs)[-1][1]
        logger.info(f"üîÅ Ph√°t hi·ªán checkpoint: {latest_ckpt}")

    # ---- 3Ô∏è‚É£ T·∫¢I DATASET ----
    logger.info("üì• ƒêang t·∫£i dataset ti·∫øng Vi·ªát (streaming)...")
    dataset_id = "VTSNLP/vietnamese_curated_dataset"

    # ---- 4Ô∏è‚É£ KH·ªûI T·∫†O HO·∫∂C T·∫¢I M√î H√åNH ----
    if latest_ckpt:
        model = Word2Vec.load(latest_ckpt)
        start_epoch = int(re.findall(r"epoch_(\d+)", latest_ckpt)[0]) + 1
        logger.info(f"üîÑ Ti·∫øp t·ª•c hu·∫•n luy·ªán t·ª´ epoch {start_epoch}")
        # === B·ªî SUNG: Ghi ƒë√® tham s·ªë workers sau khi t·∫£i checkpoint ===
        if model.workers != WORKERS:
            TARGET_WORKERS = WORKERS
            model.workers = TARGET_WORKERS
            logger.info(f"‚ö° ƒê√£ ghi ƒë√® workers t·ª´ {model.workers} c≈© sang {TARGET_WORKERS} m·ªõi ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng.")
    else:
        model = Word2Vec(vector_size=300, window=5, min_count=10, sg=1, workers=12)
        start_epoch = 1
        logger.info("‚ú® Kh·ªüi t·∫°o m√¥ h√¨nh m·ªõi.")

        # --- build vocab ---
        ds_stream_vocab = load_dataset(dataset_id, split="train")
        logger.info("üî® ƒêang x√¢y vocabulary...")
        model.build_vocab(
            tqdm(tokenize_stream(ds_stream_vocab, trie, single_stop, max_phrase_len),
                 desc="X√¢y vocab (streaming)")
        )
        logger.info(f"‚úÖ ƒê√£ x√¢y vocab: {len(model.wv.key_to_index):,} t·ª´.")

    # ---- 5Ô∏è‚É£ HU·∫§N LUY·ªÜN THEO EPOCH ----
    num_epochs = 5
    for epoch in range(start_epoch, num_epochs + 1):
        logger.info(f"üîÅ Epoch {epoch}/{num_epochs}")
        ds_stream = load_dataset(dataset_id, split="train")
        model.train(
            tqdm(tokenize_stream(ds_stream, trie, single_stop, max_phrase_len),
                 desc=f"Hu·∫•n luy·ªán epoch {epoch}"),
            total_examples=model.corpus_count,
            total_words=model.corpus_total_words,
            epochs=1,
            callbacks=[EpochSaver(CHECKPOINT_PREFIX)]
        )

    # ---- 6Ô∏è‚É£ L∆ØU M√î H√åNH CU·ªêI ----
    model.save(FINAL_MODEL_PATH)
    logger.info(f"üíæ M√¥ h√¨nh cu·ªëi c√πng ƒë∆∞·ª£c l∆∞u t·∫°i: {FINAL_MODEL_PATH}")

    # ---- 7Ô∏è‚É£ TEST NHANH ----
    for word in ["ƒë·∫πp", "ƒÉn", "u·ªëng", "tr∆∞·ªùng", "y√™u"]:
        if word in model.wv:
            sim = model.wv.most_similar(word, topn=5)
            logger.info(f"T·ª´ g·∫ßn '{word}': {sim}")
        else:
            logger.info(f"T·ª´ '{word}' ch∆∞a ƒë·ªß t·∫ßn su·∫•t xu·∫•t hi·ªán.")

# ==========================
# ‚ñ∂Ô∏è ENTRY POINT
# ==========================
if __name__ == "__main__":
    main()
