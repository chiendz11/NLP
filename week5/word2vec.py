import logging
import os
import time
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec

# ==================================
# ‚öôÔ∏è C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N & THAM S·ªê
# ==================================
OUTPUT_DIR = r"D:\Vietnamese_Word2Vec"
CORPUS_PATH = os.path.join(OUTPUT_DIR, "vietnamese_corpus_tokenized.txt")
MODEL_PATH = os.path.join(OUTPUT_DIR, "word2vec_model_final.bin")
CHECKPOINT_PATH_FORMAT = os.path.join(OUTPUT_DIR, "word2vec_model_epoch_{}.bin")

VECTOR_SIZE = 300
WINDOW = 5
MIN_COUNT = 5
WORKERS = os.cpu_count() - 2 or 4
EPOCHS = 15

# ==================================
# üß© C·∫§U H√åNH LOGGING
# ==================================
logging.basicConfig(
    format="%(asctime)s : %(levelname)s : %(message)s",
    level=logging.INFO
)

# ==================================
# ‚ö° FAST LINE SENTENCE (ƒê·ªåC NHANH H∆†N)
# ==================================
class FastLineSentence:
    """
    Generator ƒë·ªçc file corpus c·ª±c nhanh v·ªõi buffer l·ªõn (128MB).
    Ti·∫øt ki·ªám RAM, t·∫≠n d·ª•ng SSD/OS cache hi·ªáu qu·∫£.
    """
    def __init__(self, fname, encoding="utf8", buffer_size=128 * 1024 * 1024):
        self.fname = fname
        self.encoding = encoding
        self.buffer_size = buffer_size

    def __iter__(self):
        total_bytes = os.path.getsize(self.fname)
        read_bytes = 0
        start_time = time.time()

        with open(self.fname, "r", encoding=self.encoding, buffering=self.buffer_size) as f:
            for line in f:
                read_bytes += len(line.encode("utf-8"))
                yield line.strip().split()

                # Log ti·∫øn ƒë·ªô ƒë·ªçc (m·ªói 1GB)
                if read_bytes % (1_000_000_000) < 100_000:
                    elapsed = time.time() - start_time
                    elapsed = max(elapsed, 1e-9)  # tr√°nh chia cho 0
                    speed = read_bytes / (1024 * 1024 * elapsed)
                    logging.info(f"üìñ ƒê√£ ƒë·ªçc {(read_bytes / 1_000_000_000):.2f} GB / {(total_bytes / 1_000_000_000):.2f} GB "
                                 f"({speed:.1f} MB/s)")

# ==================================
# üéì CALLBACK L∆ØU CHECKPOINT
# ==================================
class EpochSaver(CallbackAny2Vec):
    def __init__(self, start_time, checkpoint_path_format):
        self.epoch = 0
        self.start_time = start_time
        self.checkpoint_path_format = checkpoint_path_format

    def on_epoch_begin(self, model):
        logging.info(f"üöÄ B·∫Øt ƒë·∫ßu Epoch {self.epoch + 1}...")

    def on_epoch_end(self, model):
        self.epoch += 1
        elapsed = time.time() - self.start_time
        checkpoint_path = self.checkpoint_path_format.format(self.epoch)
        model.save(checkpoint_path)
        logging.info(f"‚úÖ Ho√†n th√†nh Epoch {self.epoch}. ƒê√£ l∆∞u checkpoint t·∫°i: {checkpoint_path}")
        logging.info(f"‚è±Ô∏è Th·ªùi gian t·ªïng: {elapsed:.2f}s")

# ==================================
# üß† HU·∫§N LUY·ªÜN WORD2VEC
# ==================================
def train_word2vec():
    if not os.path.exists(CORPUS_PATH):
        logging.error(f"‚ùå Kh√¥ng t√¨m th·∫•y corpus t·∫°i: {CORPUS_PATH}")
        return None

    logging.info(f"üìö ƒê·ªçc corpus t·ª´: {CORPUS_PATH}")
    sentences = FastLineSentence(CORPUS_PATH)
    start_time = time.time()
    epoch_saver = EpochSaver(start_time, CHECKPOINT_PATH_FORMAT)

    logging.info(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Skip-Gram v·ªõi {WORKERS} workers...")
    model = Word2Vec(
        sentences=sentences,
        vector_size=VECTOR_SIZE,
        window=WINDOW,
        min_count=MIN_COUNT,
        workers=WORKERS,
        sg=1,  # ‚úÖ Skip-Gram
        negative=10,  # S·ªë t·ª´ √¢m (negative sampling)
        sample=1e-5,  # Ng∆∞·ª°ng downsample
        epochs=EPOCHS,
        compute_loss=True,
        callbacks=[epoch_saver]
    )

    model.save(MODEL_PATH)
    logging.info(f"üíæ Ho√†n t·∫•t to√†n b·ªô {EPOCHS} Epochs. L∆∞u m√¥ h√¨nh cu·ªëi c√πng t·∫°i: {MODEL_PATH}")
    return model

# ==================================
# üß™ KI·ªÇM TH·ª¨ M√î H√åNH
# ==================================
def test_model(model):
    print("\n" + "="*50)
    print("üåü KI·ªÇM TH·ª¨ M√î H√åNH WORD2VEC üåü")
    print("="*50)

    test_words = ["Vi·ªát_Nam", "H√†_N·ªôi", "gi√°o_d·ª•c", "Covid-19", "ƒëi·ªán_tho·∫°i"]
    for word in test_words:
        if word in model.wv:
            print(f"\nüëâ {word}:")
            for rank, (w, score) in enumerate(model.wv.most_similar(word, topn=10)):
                print(f"   {rank+1}. {w:<20} ({score:.4f})")
        else:
            print(f"\n‚ö†Ô∏è '{word}' kh√¥ng ƒë·ªß t·∫ßn su·∫•t (min_count={MIN_COUNT}).")

# ==================================
# ‚ñ∂Ô∏è CH·∫†Y CH∆Ø∆†NG TR√åNH
# ==================================
if __name__ == "__main__":
    trained_model = train_word2vec()
    if trained_model:
        test_model(trained_model)
