# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UYED-soFZLHB105d-WCXww3YoXb3UQ0K
"""

import string
import numpy as np
import re
import sys
!pip install tensorflow numpy
# THAY THẾ: Import TensorFlow/Keras thay vì PyTorch
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Lambda, Dense
from tensorflow.keras import backend as K

# ======================================================================
# LƯU Ý QUAN TRỌNG: CÀI ĐẶT THƯ VIỆN CẦN THIẾT
# ======================================================================
# Chạy các lệnh sau trong các cell riêng biệt TRƯỚC KHI CHẠY CODE NÀY:


# ----------------------------------------------------------------------
# 1. Load Dataset
# ----------------------------------------------------------------------
URL_KIEU = "https://raw.githubusercontent.com/duyet/truyenkieu-word2vec/master/truyen_kieu_data.txt"
print(f"Bắt đầu tải file từ RAW URL: {URL_KIEU}")
!wget -O truyen_kieu.txt {URL_KIEU}
print("\nĐã tải file truyen_kieu.txt.")

# ----------------------------------------------------------------------
# 2. Preprocessing
# ----------------------------------------------------------------------
PUNCT_TO_REMOVE = string.punctuation + string.digits

def clean_text(text):
    """
    Hàm làm sạch: loại bỏ dấu câu, chữ số và chuyển về chữ thường.
    """
    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Đọc file và làm sạch toàn bộ văn bản
raw_lines = []
try:
    with open('truyen_kieu.txt', 'r', encoding='utf-8') as f:
        raw_lines = f.readlines()
except FileNotFoundError:
    print("LỖI CẤP THIẾT: Không tìm thấy file truyen_kieu.txt sau khi tải.")
    sys.exit(1)

# Kết hợp tất cả các dòng đã làm sạch thành một corpus để Tokenizer hoạt động hiệu quả
corpus = [clean_text(line.strip()) for line in raw_lines if line.strip()]

# Tạo danh sách tokens (chỉ cần thiết cho bước 3)
cleaned_text = " ".join(corpus)
tokens = [word for word in cleaned_text.split() if word]

print("--- Kết quả Tiền xử lý ---")
print(f"Tổng số từ sau làm sạch: {len(tokens)}")

# ======================================================================
# 3. BUILD DATA (Xây dựng Dữ liệu CBOW)
# ======================================================================
# Xây dựng Từ điển (Vocabulary) - Vẫn sử dụng logic token thủ công để lấy centers/contexts
WINDOW_SIZE = 2

def get_centers_and_contexts(tokens, max_window_size):
    # Sử dụng centers và contexts làm danh sách từ (string)
    centers = []
    contexts = []
    for i in range(max_window_size, len(tokens) - max_window_size):
        center_word = tokens[i]
        context_words = tokens[i - max_window_size : i] + tokens[i + 1 : i + max_window_size + 1]
        centers.append(center_word)
        contexts.append(context_words)
    return centers, contexts

centers, contexts = get_centers_and_contexts(tokens, WINDOW_SIZE)

print("\n--- Xây dựng Từ điển và Cặp Dữ liệu ---")
print(f"Tổng số cặp dữ liệu huấn luyện: {len(centers)}")

# ======================================================================
# 4. REPRESENTATION (Mã hóa và Chuẩn hóa Input/Output Keras)
# ======================================================================

# Tham số theo ảnh
MAX_LENGTH = 4 # Kích thước input (2 * WINDOW_SIZE)
EMBEDDING_SIZE = 200 # Kích thước vector nhúng

# 1. Tạo và fit Tokenizer
# oov_token='<OOV>' được thêm vào để xử lý các từ không có trong từ điển (tùy chọn)
tokenizer = Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(corpus) # Fit trên toàn bộ corpus (các dòng đã làm sạch)

# Kích thước từ điển (bao gồm 1 cho padding và 1 cho OOV nếu có)
VOCAB_SIZE = len(tokenizer.word_index) + 1
print(f"Kích thước Từ điển (Vocabulary Size): {VOCAB_SIZE}")

# 2. Mã hóa Input (Contexts)
# texts_to_sequences chuyển danh sách từ thành danh sách chỉ số
train_seq = tokenizer.texts_to_sequences(contexts)

# Pad sequences (chuẩn hóa độ dài input)
train_seq_pad = pad_sequences(
    train_seq,
    maxlen=MAX_LENGTH,
    padding='post',
    truncating='post'
)
print(f"Kích thước Input (train_seq_pad): {train_seq_pad.shape}")

# 3. Mã hóa Output (Centers/Labels)
# Lấy chỉ số cho từng Center
train_labels_indices = [tokenizer.word_index[label] for label in centers]

# Chuyển chỉ số thành One-hot Encoding (train_labels)
# to_categorical cần VOCAB_SIZE làm num_classes
train_labels = to_categorical(train_labels_indices, num_classes=VOCAB_SIZE)

print(f"Kích thước Output (train_labels): {train_labels.shape}")


# ======================================================================
# 5. CBOW MODEL (Xây dựng Mô hình Keras Sequential)
# ======================================================================

# Định nghĩa hàm Lambda để tính trung bình các vector Embedding
# K.mean(x, axis=1) tính trung bình trên trục chiều thứ 2 (vector nhúng)
def cbow_mean(x):
    return K.mean(x, axis=1)

# Xây dựng mô hình Sequential
cbow = Sequential()

# Lớp 1: Embedding
cbow.add(Embedding(
    input_dim=VOCAB_SIZE,
    output_dim=EMBEDDING_SIZE,
    input_length=MAX_LENGTH,
    name='embedding_layer'
))

# Lớp 2: Lambda (Tính trung bình: Bag-of-Words)
cbow.add(Lambda(cbow_mean, output_shape=(EMBEDDING_SIZE,)))

# Lớp 3: Dense (Output Layer)
# Activation: 'softmax' để có phân phối xác suất trên toàn bộ từ điển
cbow.add(Dense(VOCAB_SIZE, activation='softmax'))

# In tóm tắt mô hình (tương tự như trong ảnh)
cbow.summary()


# ----------------------------------------------------------------------
# 6. PREDICT (Compile và Huấn luyện)
# ----------------------------------------------------------------------

# Compile mô hình
cbow.compile(
    loss='categorical_crossentropy', # Phù hợp với One-hot encoding
    optimizer='adam',
    metrics=['accuracy']
)

# Huấn luyện mô hình (Fit)
NUM_EPOCHS = 30 # Sử dụng 30 epochs như trong ảnh

print(f"\n--- Bắt đầu Huấn luyện Mô hình Keras CBOW ({NUM_EPOCHS} epochs) ---")
# Sử dụng train_seq_pad làm input và train_labels làm output
history = cbow.fit(
    train_seq_pad,
    train_labels,
    epochs=NUM_EPOCHS,
    verbose=1
)

print("\nQuá trình huấn luyện hoàn tất.")
# Dữ liệu mẫu cần dự đoán
SAMPLE_TEXT = 'trăm năm cõi người'
print(f"Ngữ cảnh đầu vào: '{SAMPLE_TEXT}'")

# 1. Mã hóa ngữ cảnh mẫu thành chuỗi index (sequences)
sample_seq = tokenizer.texts_to_sequences([SAMPLE_TEXT.split()])

# 2. Chuẩn hóa chuỗi index (padding)
# Đảm bảo maxlen, padding, và truncating giống như khi huấn luyện
sample_seq_pad = pad_sequences(
    sample_seq,
    maxlen=MAX_LENGTH,
    padding='post',
    truncating='post'
)
print(f"Vector Input (Index): {sample_seq_pad}")

# 3. Dự đoán (predict)
# cbow.predict trả về vector xác suất (logits sau softmax)
prediction_probs = cbow.predict(sample_seq_pad)

# 4. Lấy index có xác suất cao nhất
# np.argmax tìm chỉ số (index) có giá trị cao nhất
predicted_index = np.argmax(prediction_probs)

# 5. Chuyển index thành từ (word)
predicted_word = tokenizer.index_word[predicted_index]

print(f"\nVector xác suất dự đoán (10 giá trị đầu): {prediction_probs[0][:10]}...")
print(f"Index dự đoán (Xác suất cao nhất): {predicted_index}")
print(f"Từ dự đoán (Kết quả): '{predicted_word}'")

