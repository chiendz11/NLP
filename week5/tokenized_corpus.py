import re
import os
import unicodedata
import logging
import time
from tqdm import tqdm
from datasets import load_dataset
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, as_completed
from underthesea import word_tokenize

# ==========================
# ‚öôÔ∏è C·∫§U H√åNH CHUNG
# ==========================
OUTPUT_DIR = r"D:\Vietnamese_Word2Vec"
os.makedirs(OUTPUT_DIR, exist_ok=True)
# File n√†y s·∫Ω ch·ª©a c√°c token, v·ªõi t·ª´ gh√©p ƒë∆∞·ª£c n·ªëi b·∫±ng "_"
CORPUS_PATH = os.path.join(OUTPUT_DIR, "vietnamese_corpus_tokenized.txt")

# ==========================
# üß© C·∫§U H√åNH LOGGING
# ==========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s : %(levelname)s : %(message)s"
)
logger = logging.getLogger(__name__)

# ==========================
# üßπ CLEANING & VALIDATION
# ==========================
_re_non_vn = re.compile(r"[^0-9a-zA-Z√Ä-·ª¥√†-·ªµƒëƒê\s]", re.UNICODE)
_re_multi_space = re.compile(r"\s+")
# Cho ph√©p k√Ω t·ª± '_' ƒë·ªÉ kh·ªõp v·ªõi c√°c token ƒëa t·ª´ (multi-word tokens)
_re_valid_vn_token = re.compile(
    r"^[a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ_]+$",
    re.UNICODE
)

def normalize_text(text):
    """Chu·∫©n ho√° Unicode v√† lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá."""
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize("NFC", text)
    text = _re_non_vn.sub(" ", text)
    text = _re_multi_space.sub(" ", text)
    return text.strip().lower()

def is_valid_vietnamese_token(word):
    """Ki·ªÉm tra token c√≥ h·ª£p l·ªá ti·∫øng Vi·ªát, ch·∫•p nh·∫≠n d·∫•u g·∫°ch d∆∞·ªõi."""
    return bool(_re_valid_vn_token.match(word))

# ==========================
# üß± TRIE STOPWORDS (GI·ªÆ NGUY√äN)
# ==========================
def build_trie(phrases):
    """T·∫°o Trie cho stopwords. Gi·∫£ ƒë·ªãnh input phrases ƒë√£ c√≥ d·∫•u '_' n·∫øu l√† t·ª´ gh√©p."""
    trie = {}
    max_len = 0
    single_word_set = set()
    for phr in phrases:
        # T√°ch phrase b·∫±ng kho·∫£ng tr·∫Øng. V√≠ d·ª•: "b·ªüi_v√¨ th·∫ø_n√™n" -> ['b·ªüi_v√¨', 'th·∫ø_n√™n']
        toks = phr.split()
        if not toks:
            continue
        
        if len(toks) == 1:
            single_word_set.add(toks[0])
            max_len = max(max_len, 1)
            continue
            
        node = trie
        for tok in toks:
            node = node.setdefault(tok, {})
        node["_end"] = True
        max_len = max(max_len, len(toks))
    return trie, single_word_set, max_len

# ==========================
# üß† TOKENIZE + STOPWORD FILTER
# ==========================
def tokenize_and_clean(text, trie, single_stop, max_phrase_len):
    """T√°ch t·ª´, lo·∫°i stopword & t·ª´ r√°c, l√†m vi·ªác tr·ª±c ti·∫øp v·ªõi token c√≥ '_'."""
    text = normalize_text(text)
    if not text:
        return ""
    
    # 1. Tokenize (Output: list of tokens with '_')
    tokenized_str = word_tokenize(text, format="text")
    tokens = [t for t in tokenized_str.split() if is_valid_vietnamese_token(t)]

    if not tokens:
        return ""

    # 2. L·ªçc Stopword b·∫±ng Trie
    filtered = []
    i, n = 0, len(tokens)
    
    while i < n:
        node = trie
        j = i
        steps = 0
        match_len = 0
        
        # T√¨m match cho c·ª•m stopword
        while j < n and steps < max_phrase_len:
            tok = tokens[j]
            if not isinstance(node, dict) or tok not in node:
                break
            node = node[tok]
            j += 1
            steps += 1
            if node.get("_end"):
                match_len = j - i
        
        # L·ªçc v√† ghi k·∫øt qu·∫£
        is_stop_phrase = match_len > 0
        is_single_stop_word = match_len == 0 and tokens[i] in single_stop
        
        if not is_stop_phrase and not is_single_stop_word:
            filtered.append(tokens[i])

        i += max(match_len, 1)

    return " ".join(filtered)

# ==========================
# üîÄ MULTIPROCESS TOKENIZATION
# ==========================
def process_batch(batch_texts, trie, single_stop, max_phrase_len, batch_index):
    """H√†m ch·∫°y trong ti·∫øn tr√¨nh con ƒë·ªÉ x·ª≠ l√Ω m·ªôt batch vƒÉn b·∫£n."""
    start = time.time()
    results = []
    for text in batch_texts:
        # G·ªçi h√†m x·ª≠ l√Ω (tokenize_and_clean)
        clean_line = tokenize_and_clean(text, trie, single_stop, max_phrase_len)
        if clean_line:
            results.append(clean_line)
    elapsed = time.time() - start
    logger.info(f"üß© Batch {batch_index}: {len(batch_texts):,} d√≤ng trong {elapsed:.2f}s ({len(batch_texts)/elapsed:.1f} doc/s)")
    return results

# ==========================
# üöÄ MAIN (STREAMING + MANUAL MULTIPROCESS)
# ==========================
def main():
    # ---- 1Ô∏è‚É£ KI·ªÇM TRA FILE V√Ä THI·∫æT L·∫¨P CH·∫æ ƒê·ªò N·ªêI TI·∫æP ----
    skip_count = 0
    file_mode = "w" # M·∫∑c ƒë·ªãnh l√† ghi m·ªõi

    if os.path.exists(CORPUS_PATH) and os.path.getsize(CORPUS_PATH) > 0:
        # ƒê·∫øm s·ªë d√≤ng ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥ (C∆° ch·∫ø Continuation)
        try:
            with open(CORPUS_PATH, "r", encoding="utf-8") as f:
                # C√°ch ƒë·∫øm d√≤ng hi·ªáu qu·∫£ h∆°n cho file l·ªõn
                skip_count = sum(1 for line in f) 
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë·∫øm d√≤ng: {e}. B·∫Øt ƒë·∫ßu ghi m·ªõi (w).")
            # Gi·ªØ skip_count = 0 v√† file_mode = "w"
        
        if skip_count > 0:
            file_mode = "a" # Chuy·ªÉn sang ch·∫ø ƒë·ªô n·ªëi th√™m
            logger.warning(f"‚ö†Ô∏è Ph√°t hi·ªán ti·∫øn tr√¨nh b·ªã gi√°n ƒëo·∫°n. {skip_count:,} d√≤ng ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.")
            logger.warning(f"S·∫Ω ti·∫øp t·ª•c x·ª≠ l√Ω t·ª´ m·∫´u th·ª© {skip_count + 1} v√† ghi N·ªêI TH√äM (append) v√†o file.")
        else:
            # File c√≥ size > 0 nh∆∞ng ƒë·∫øm d√≤ng = 0 (c√≥ th·ªÉ do l·ªói ghi), v·∫´n ghi m·ªõi
            logger.info("File corpus t·ªìn t·∫°i nh∆∞ng tr·ªëng ho·∫∑c l·ªói ƒë·∫øm d√≤ng. B·∫Øt ƒë·∫ßu ghi m·ªõi (w).")


    # ---- 2Ô∏è‚É£ STOPWORDS ----
    stopword_file = "stopwords.csv"
    try:
        df = pd.read_csv(stopword_file)
    except FileNotFoundError:
        logger.error(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file stopword: {stopword_file}")
        logger.error("Vui l√≤ng ƒë·∫£m b·∫£o file 'stopwords.csv' t·ªìn t·∫°i trong c√πng th∆∞ m·ª•c.")
        return
    
    stop_phrases = [str(w).strip() for w in df["stopwords"].dropna()]
    trie, single_stop, max_phrase_len = build_trie(stop_phrases)
    logger.info(f"‚úÖ ƒê√£ t·∫£i {len(stop_phrases)} c·ª•m stopword.")

    # ---- 3Ô∏è‚É£ DATASET STREAMING ----
    dataset_id = "VTSNLP/vietnamese_curated_dataset"
    # D√πng Streaming ƒë·ªÉ gi·ªØ RAM an to√†n (B·∫ÆT BU·ªòC ƒë·ªÉ tr√°nh crash)
    dataset = load_dataset(dataset_id, split="train", streaming=True)
    logger.info("üì° ƒêang d√πng ch·∫ø ƒë·ªô STREAMING (RAM an to√†n).")

    # B·ªé QUA C√ÅC D√íNG ƒê√É X·ª¨ L√ù N·∫æU ƒêANG TI·∫æP T·ª§C
    if skip_count > 0:
        logger.info(f"‚è≠Ô∏è B·ªè qua {skip_count:,} m·∫´u ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.")
        dataset = dataset.skip(skip_count)
        
    # ---- 4Ô∏è‚É£ TOKENIZE SONG SONG ----
    num_workers = max(1, os.cpu_count() - 2) 
    batch_size = 15000
    
    logger.info(f"‚ö° S·ª≠ d·ª•ng {num_workers} ti·∫øn tr√¨nh. Batch size: {batch_size}")

    start_time = time.time()
    processed_count = skip_count # Kh·ªüi t·∫°o processed_count b·∫±ng s·ªë d√≤ng ƒë√£ skip
    batch_buffer = []
    batch_index = 0 # Kh√¥ng reset batch_index ƒë·ªÉ log v·∫´n ch·∫°y tu·∫ßn t·ª±
    max_futures_limit = num_workers + 1

    with tqdm(total=None, desc="T·ªïng ti·∫øn ƒë·ªô (d√≤ng/s)", unit=" lines") as pbar: 
        # C·∫≠p nh·∫≠t pbar ƒë·ªÉ ph·∫£n √°nh ti·∫øn ƒë·ªô ƒë√£ x·ª≠ l√Ω
        pbar.n = processed_count 
        pbar.refresh()
        
        # M·ªü file ·ªü ch·∫ø ƒë·ªô ƒë√£ x√°c ƒë·ªãnh ('w' ho·∫∑c 'a')
        with open(CORPUS_PATH, file_mode, encoding="utf-8") as out_f:
            with ProcessPoolExecutor(max_workers=num_workers) as executor:
                futures = set()
                try:
                    # ƒê·ªçc t·ª´ng m·∫´u t·ª´ stream v√† G·ª¨I T√ÅC V·ª§
                    for sample in dataset:
                        text = sample.get("text", "")
                        if not text:
                            continue
                        batch_buffer.append(text)
                        
                        if len(batch_buffer) >= batch_size:
                            batch_index += 1
                            future = executor.submit(process_batch, batch_buffer, trie, single_stop, max_phrase_len, batch_index)
                            futures.add(future)
                            batch_buffer = []

                            # THU TH·∫¨P K·∫æT QU·∫¢
                            if len(futures) > max_futures_limit:
                                done = next(as_completed(futures))
                                futures.remove(done)
                                
                                results = done.result()
                                # Ghi k·∫øt qu·∫£ v√†o file ƒë√£ m·ªü ·ªü ch·∫ø ƒë·ªô 'a'
                                out_f.write("\n".join(results) + "\n") 
                                count = len(results)
                                processed_count += count
                                pbar.update(count)

                                elapsed = time.time() - start_time
                                logger.info(f"‚úÖ T·ªîNG: {processed_count:,} d√≤ng ƒë√£ x·ª≠ l√Ω ({processed_count/elapsed:.1f} doc/s)")
                                
                except Exception as e:
                    logger.error(f"‚ùå L·ªói x·∫£y ra trong qu√° tr√¨nh ƒë·ªçc stream ho·∫∑c x·ª≠ l√Ω: {e}")
                    
                # ---- 5Ô∏è‚É£ GIAI ƒêO·∫†N HO√ÄN T·∫§T ----
                
                # 5.1 X·ª≠ l√Ω batch cu·ªëi (n·∫øu c√≥)
                if batch_buffer:
                    batch_index += 1
                    future = executor.submit(process_batch, batch_buffer, trie, single_stop, max_phrase_len, batch_index)
                    futures.add(future)

                pbar.set_description(f"Ch·ªù {len(futures)} Worker cu·ªëi") 

                # 5.2 Ch·ªù t·∫•t c·∫£ futures c√≤n l·∫°i ho√†n th√†nh
                for future in as_completed(futures):
                     try:
                        results = future.result()
                        out_f.write("\n".join(results) + "\n")
                        count = len(results)
                        processed_count += count
                        pbar.update(count)
                     except Exception as worker_e:
                        logger.error(f"‚ùå L·ªói Worker: {worker_e}")


    elapsed = time.time() - start_time
    logger.info(f"‚úÖ Ho√†n t·∫•t! Tokenized {processed_count:,} d√≤ng trong {elapsed:.2f}s "
                f"({processed_count/elapsed:.1f} doc/s)")
    logger.info(f"üíæ File corpus ƒë∆∞·ª£c l∆∞u t·∫°i: {CORPUS_PATH}")

# ==========================
# ‚ñ∂Ô∏è ENTRY POINT
# ==========================
if __name__ == "__main__":
    main()
